"""
Lambda Function: SQS to AWS Batch Trigger (Safe Version)
ì•ˆì „ì¥ì¹˜:
1. Lambda Concurrency = 1 (ë™ì‹œ ì‹¤í–‰ ë°©ì§€)
2. ì‹¤í–‰ ì¤‘ì¸ Job ì²´í¬ (ì¤‘ë³µ ì œì¶œ ë°©ì§€)
"""

import json
import boto3
import os
import logging
from datetime import datetime
from botocore.exceptions import ClientError

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
batch_client = boto3.client('batch')

# í™˜ê²½ ë³€ìˆ˜
JOB_QUEUE = os.environ['BATCH_JOB_QUEUE']
JOB_DEFINITION = os.environ['BATCH_JOB_DEFINITION']
MAX_CONCURRENT_JOBS = int(os.environ.get('MAX_CONCURRENT_JOBS', '1'))

# PostgreSQL í™˜ê²½ ë³€ìˆ˜ (Job Definitionì— ìˆëŠ” ê°’ì„ Lambdaì—ì„œë„ ê°€ì ¸ì˜´)
POSTGRES_HOST = os.environ.get('POSTGRES_HOST', '')
POSTGRES_DB = os.environ.get('POSTGRES_DB', '')
POSTGRES_USER = os.environ.get('POSTGRES_USER', '')


def check_running_jobs():
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Batch Job ìˆ˜ í™•ì¸
    RUNNING, RUNNABLE, STARTING ìƒíƒœ ëª¨ë‘ í¬í•¨
    
    Returns:
        int: í™œì„± ìƒíƒœì¸ Job ìˆ˜
    """
    total_jobs = 0
    
    try:
        # RUNNING, RUNNABLE, STARTING ìƒíƒœ ëª¨ë‘ í™•ì¸
        for status in ['RUNNING', 'RUNNABLE', 'STARTING']:
            response = batch_client.list_jobs(
                jobQueue=JOB_QUEUE,
                jobStatus=status
            )
            job_count = len(response.get('jobSummaryList', []))
            total_jobs += job_count
            
            if job_count > 0:
                logger.info(f"Jobs in {status} state: {job_count}")
        
        logger.info(f"Total active jobs: {total_jobs}")
        
    except Exception as e:
        logger.error(f"Failed to check running jobs: {str(e)}")
        # ì—ëŸ¬ ì‹œì—ëŠ” ì•ˆì „í•˜ê²Œ 0 ë°˜í™˜ (Job ì œì¶œ í—ˆìš©)
        total_jobs = 0
    
    return total_jobs


def parse_s3_event(body):
    """
    SQS ë©”ì‹œì§€ì—ì„œ S3 ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ
    
    Args:
        body: SQS ë©”ì‹œì§€ body (JSON string ë˜ëŠ” dict)
    
    Returns:
        tuple: (bucket, key) ë˜ëŠ” (None, None)
    """
    try:
        # JSON stringì¸ ê²½ìš° íŒŒì‹±
        if isinstance(body, str):
            body = json.loads(body)
        
        # S3 Event Notification í˜•ì‹
        if 'Records' in body and len(body['Records']) > 0:
            s3_record = body['Records'][0]
            if 's3' in s3_record:
                bucket = s3_record['s3']['bucket']['name']
                key = s3_record['s3']['object']['key']
                return bucket, key
        
        # Backendì—ì„œ ë³´ë‚¸ ë©”ì‹œì§€ í˜•ì‹ (eventType: video-uploaded)
        if 's3' in body and isinstance(body['s3'], dict):
            if 'bucket' in body['s3'] and 'key' in body['s3']:
                return body['s3']['bucket'], body['s3']['key']
        
        # ì§ì ‘ ì „ì†¡ëœ ë©”ì‹œì§€ í˜•ì‹ (ë ˆê±°ì‹œ)
        if 'bucket' in body and 'key' in body:
            return body['bucket'], body['key']
        
        logger.warning(f"Unknown message format: {body}")
        return None, None
        
    except Exception as e:
        logger.error(f"Failed to parse S3 event: {str(e)}")
        return None, None


def lambda_handler(event, context):
    """
    Lambda í•¸ë“¤ëŸ¬ - SQS ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ AWS Batch Job ì œì¶œ
    
    ì•ˆì „ì¥ì¹˜:
    1. ì‹¤í–‰ ì¤‘ì¸ Jobì´ ìˆìœ¼ë©´ ì œì¶œí•˜ì§€ ì•ŠìŒ
    2. MAX_CONCURRENT_JOBS ì´ìƒì´ë©´ ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ íë¡œ ë°˜í™˜
    """
    logger.info(f"Received {len(event['Records'])} messages from SQS")
    
    # ì•ˆì „ì¥ì¹˜: ì‹¤í–‰ ì¤‘ì¸ Job ì²´í¬
    running_jobs = check_running_jobs()
    
    if running_jobs >= MAX_CONCURRENT_JOBS:
        logger.warning(
            f"Too many jobs running ({running_jobs}/{MAX_CONCURRENT_JOBS}), "
            f"returning messages to queue"
        )
        # ëª¨ë“  ë©”ì‹œì§€ë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œí•˜ì—¬ ë‹¤ì‹œ íë¡œ ë°˜í™˜
        return {
            'batchItemFailures': [
                {'itemIdentifier': record['messageId']} 
                for record in event['Records']
            ]
        }
    
    # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
    failed_messages = []
    successful_count = 0
    
    for record in event['Records']:
        message_id = record['messageId']
        
        try:
            # SQS ë©”ì‹œì§€ íŒŒì‹±
            body = record['body']
            bucket, key = parse_s3_event(body)
            
            if not bucket or not key:
                logger.error(f"Invalid message format: {body}")
                failed_messages.append({'itemIdentifier': message_id})
                continue
            
            logger.info(f"Processing video: s3://{bucket}/{key}")
            
            # AWS Batch Job ì œì¶œ
            try:
                # VIDEO_ID ì¶”ì¶œ: ìš°ì„ ìˆœìœ„
                # 1. SQS ë©”ì‹œì§€ì˜ video.id í•„ë“œ
                # 2. MessageAttributesì˜ video_id
                # 3. S3 key ê²½ë¡œì—ì„œ ì¶”ì¶œ (videos/{video_id}/{filename})
                # 4. Fallback: S3 keyì—ì„œ íŒŒì¼ëª… ì‚¬ìš©
                video_id = None
                
                # 1. ë©”ì‹œì§€ bodyì—ì„œ video.id ì°¾ê¸°
                try:
                    body_dict = json.loads(body) if isinstance(body, str) else body
                    if 'video' in body_dict and 'id' in body_dict['video']:
                        video_id = str(body_dict['video']['id'])
                        logger.info(f"Extracted video_id from message body: {video_id}")
                except Exception as e:
                    logger.debug(f"Could not extract video_id from body: {e}")
                
                # 2. MessageAttributesì—ì„œ ì°¾ê¸°
                if not video_id and 'messageAttributes' in record:
                    attrs = record['messageAttributes']
                    if 'video_id' in attrs:
                        video_id = attrs['video_id'].get('stringValue', attrs['video_id'].get('StringValue'))
                        logger.info(f"Extracted video_id from MessageAttributes: {video_id}")
                
                # 3. S3 key ê²½ë¡œì—ì„œ ì¶”ì¶œ: videos/{video_id}/{filename}
                if not video_id:
                    try:
                        # S3 key í˜•ì‹: videos/123/test.mp4 â†’ video_id = 123
                        key_parts = key.split('/')
                        if len(key_parts) >= 2 and key_parts[0] == 'videos':
                            extracted_id = key_parts[1]
                            # ìˆ«ìì¸ì§€ í™•ì¸
                            if extracted_id.isdigit():
                                video_id = extracted_id
                                logger.info(f"Extracted video_id from S3 key path: {video_id}")
                            else:
                                logger.warning(f"S3 key path segment is not a number: {extracted_id}")
                    except Exception as e:
                        logger.debug(f"Could not extract video_id from S3 key path: {e}")
                
                # 4. Fallback: S3 keyì—ì„œ íŒŒì¼ëª… ì‚¬ìš© (ìˆ«ìë§Œ ì¶”ì¶œ)
                if not video_id:
                    filename = key.split('/')[-1].split('.')[0]
                    # íŒŒì¼ëª…ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„
                    import re
                    numbers = re.findall(r'\d+', filename)
                    if numbers:
                        video_id = numbers[-1]  # ë§ˆì§€ë§‰ ìˆ«ì ê·¸ë£¹ ì‚¬ìš©
                        logger.warning(f"Using extracted number from filename as video_id (fallback): {video_id}")
                    else:
                        video_id = filename
                        logger.warning(f"Using filename as video_id (fallback): {video_id}")
                
                logger.info(f"âœ… Final video_id: {video_id}")
                
                # ğŸ¯ Job ì´ë¦„ ìƒì„±: video_idë§Œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ deterministicí•˜ê²Œ)
                # íƒ€ì„ìŠ¤íƒ¬í”„ë‚˜ UUIDë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                job_name = f"video-process-{video_id}"
                logger.info(f"ğŸš€ Submitting job: {job_name}")
                
                # ğŸ”„ ê°•í™”ëœ ì¤‘ë³µ Job ë°©ì§€: job nameìœ¼ë¡œ 1ì°¨, S3 keyë¡œ 2ì°¨ í™•ì¸
                duplicate_found = False
                try:
                    # ëª¨ë“  active ìƒíƒœì˜ Job ì¡°íšŒ
                    active_statuses = ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']
                    all_active_jobs = []
                    
                    for status in active_statuses:
                        response = batch_client.list_jobs(
                            jobQueue=JOB_QUEUE,
                            jobStatus=status,
                            maxResults=100
                        )
                        all_active_jobs.extend(response.get('jobSummaryList', []))
                    
                    logger.info(f"ğŸ“Š Total active jobs: {len(all_active_jobs)}")
                    
                    # ğŸ¯ 1ì°¨: job nameìœ¼ë¡œ ë¹ ë¥¸ ì²´í¬ (API í˜¸ì¶œ ë¶ˆí•„ìš”)
                    for job_summary in all_active_jobs:
                        if job_summary.get('jobName') == job_name:
                            logger.warning(
                                f"âš ï¸ DUPLICATE JOB DETECTED by job name! "
                                f"video_id: {video_id}, "
                                f"job_name: {job_name}, "
                                f"Existing Job ID: {job_summary['jobId']} (status: {job_summary['status']})"
                            )
                            duplicate_found = True
                            successful_count += 1  # ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (SQS ë©”ì‹œì§€ ì‚­ì œ)
                            break
                    
                    # ğŸ¯ 2ì°¨: S3 keyë¡œ ì¶”ê°€ í™•ì¸ (job nameì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„)
                    if not duplicate_found:
                        current_time = int(datetime.now().timestamp() * 1000)
                        for job_summary in all_active_jobs:
                            job_id = job_summary['jobId']
                            created_at = job_summary.get('createdAt', 0)
                            time_diff_seconds = (current_time - created_at) / 1000
                            
                            # 5ë¶„ ì´ë‚´ì— ìƒì„±ëœ Jobë§Œ í™•ì¸
                            if time_diff_seconds < 300:
                                try:
                                    # Job ìƒì„¸ ì •ë³´ ì¡°íšŒ (íƒœê·¸ í™•ì¸)
                                    job_detail = batch_client.describe_jobs(jobs=[job_id])
                                    if job_detail.get('jobs'):
                                        job_tags = job_detail['jobs'][0].get('tags', {})
                                        existing_key = job_tags.get('VideoKey', '')
                                        
                                        if existing_key == key:
                                            logger.warning(
                                                f"âš ï¸ DUPLICATE JOB DETECTED by S3 key! "
                                                f"S3 Key: {key}, "
                                                f"Existing Job ID: {job_id} (status: {job_summary['status']}, "
                                                f"created {time_diff_seconds:.0f}s ago)"
                                            )
                                            duplicate_found = True
                                            successful_count += 1  # ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (SQS ë©”ì‹œì§€ ì‚­ì œ)
                                            break
                                except Exception as detail_error:
                                    logger.debug(f"Error checking job details: {detail_error}")
                    
                    if duplicate_found:
                        logger.info("âœ‹ Skipping job submission due to duplicate detection.")
                        continue  # ë‹¤ìŒ ë©”ì‹œì§€ë¡œ
                        
                except Exception as check_error:
                    logger.warning(f"âš ï¸ Failed to check for duplicate jobs: {check_error}. Proceeding with job submission anyway.")
                
                # containerOverrides.environment ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                # Job Definitionì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ë™ì  ê°’ë§Œ commandë¡œ ì „ë‹¬
                response = batch_client.submit_job(
                    jobName=job_name,
                    jobQueue=JOB_QUEUE,
                    jobDefinition=JOB_DEFINITION,
                    containerOverrides={
                        'environment': [
                            # Lambdaì—ì„œ ì „ë‹¬í•˜ëŠ” ë™ì  ê°’ë§Œ ì¶”ê°€
                            {'name': 'VIDEO_ID', 'value': video_id},
                            {'name': 'S3_BUCKET', 'value': bucket},
                            {'name': 'S3_KEY', 'value': key},
                            {'name': 'SQS_MESSAGE_ID', 'value': message_id}
                        ]
                    },
                    tags={
                        'Environment': 'dev',
                        'Source': 'Lambda-SQS',
                        'VideoKey': key,
                        'SubmittedAt': datetime.now().strftime('%Y%m%d-%H%M%S')
                    }
                )
                
                job_id = response['jobId']
                logger.info(f"âœ… Successfully submitted job: {job_id} ({job_name})")
                successful_count += 1
                    
            except Exception as e:
                logger.error(f"âŒ Failed to submit job: {str(e)}")
                failed_messages.append({'itemIdentifier': message_id})
                    
        except Exception as e:
            logger.error(f"âŒ Failed to process message {message_id}: {str(e)}")
            # ì‹¤íŒ¨í•œ ë©”ì‹œì§€ëŠ” ë‹¤ì‹œ íë¡œ (ìë™ ì¬ì‹œë„)
            failed_messages.append({'itemIdentifier': message_id})
    
    # ê²°ê³¼ ë¡œê¹…
    logger.info(f"Processing complete: {successful_count} succeeded, {len(failed_messages)} failed")
    
    # ì‹¤íŒ¨í•œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ SQSì— ë°˜í™˜ (ì¬ì‹œë„ë¨)
    if failed_messages:
        return {'batchItemFailures': failed_messages}
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'message': f'Successfully processed {successful_count} messages',
            'timestamp': datetime.now().isoformat()
        })
    }
